% ab-tx.tex

% !TEX program = pdflatex
\documentclass[12pt, letterpaper]{article}

\input{../preamble}

\author{Hengfeng Wei \\ hfwei@nju.edu.cn}
\title{Annotated Bibliography on Transactions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\maketitle
\thispagestyle{fancy}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Books} \label{section:books}

\bib{\bibentry{Bernstein:Book1987}}
\note{\it
}

\bib{\bibentry{TIS:Book2001}}
\note{\it
  It is researchers-oriented. Highly recommended.
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Transactional Consistency Models} \label{section:tcm}

%%%%%%%%%%%%%%%%%%%%
\subsection{Frameworks} \label{ss:framework}

\bib{\bibentry{LocksAndConsistency:BookChapter1994}}
\note{\it
  This paper defines four degrees of consistency,
  attempting to show the equivalence of locking, dependency, and anomaly-based
  characterizations.
  The anomaly definitions were too vague.
  The authors continue to get criticism for that aspect of the definitions.
  Only the more mathematical definitions in terms of histories and dependency graphs
  or locking have stood the test of time.

  This is a book chapter version. Originally published in 1977.
  See also~\cite{Locking:VLDB1975}.
}

\bib{\bibentry{CritiqueIsolation:SIGMOD1995}}
\note{\it
  Defines Isolation Levels in terms of phenomena;
  Introduces new phenomena;
  Define Snapshot Isolation.
}

\bib{\bibentry{IsolationHistory:IST1997}}
\note{\it
  This paper formulates these different degrees of isolation in terms of histories,
  as in the case of the usual serialization theory
  and proposes timestamp-based protocols for different degrees of isolation.
}

\bib{\bibentry{Adya:PhDThesis1999}}
\note{\it
  This thesis presents the first implementation-independent specifications
  of existing ANSI isolation levels and a number of levels
  that are widely used in commercial systems, e.g., Cursor Stability, Snapshot Isolation.

  We use a graph-based approach to define different isolation levels
  in a simple and intuitive manner.

  The thesis describes new implementation techniques for
  supporting different weak consistency levels in distributed client-server environments.
}

\bib{\bibentry{GeneralizedIsolation:ICDE2000}}
\note{\it
  Our specifications are portable;
  they apply not only to locking implementations,
  but also to optimistic and multi-version concurrency control schemes.
  Furthermore, unlike earlier definitions,
  our new specifications handle predicates in a correct and flexible manner at all levels.

  It also discusses ``Mixing of Isolation Levels''.
}

\bib{\bibentry{HAT:VLDB2013}}
\note{\it
  In this work, we consider the problem of providing
  Highly Available Transactions (HATs):
  transactional guarantees that do not suffer unavailability
  during system partitions or incur high network latency.
  We introduce a taxonomy of highly available systems
  and analyze existing ACID isolation
  and distributed data consistency guarantees
  to identify which can and cannot be achieved in HAT systems.
  This unifies the literature on weak transactional isolation,
  replica consistency, and highly available systems.
}

\bib{\bibentry{PushPull:PLDI2015}}
\note{\it
  We present a general theory of serializability,
  unifying a wide range of transactional algorithms,
  including some that are yet to come.
  To this end, we provide a compact semantics in which
  concurrent transactions PUSH their effects into the shared view
  (or UNPUSH to recall effects)
  and PULL the effects of potentially uncommitted concurrent transactions
  into their local view (or UNPULL to detangle).
}

\bib{\bibentry{Gotsman:CONCUR2015}}
\note{\it
  we propose a framework for specifying a variety of consistency models for transactions
  uniformly and declaratively.
  Our specifications are given in the style of weak memory models,
  using structures of events and relations on them.
  The specifications are particularly concise because
  they exploit the property of atomic visibility guaranteed by many consistency models:
  either all or none of the updates by a transaction can be visible to another one.
  This allows the specifications to abstract from individual events inside transactions.
}

\bib{\bibentry{Crooks:PODC2017}}
\note{\it
  This paper introduces the first state-based formalization of isolation guarantees.
}

\bib{\bibentry{ClientCentric:PhDThesis2019}}
\note{\it
  The PhD Thesis version of~\cite{Crooks:PODC2017}.
}
%%%%%%%%%%%%%%%%%%%%
\subsection{Serializability} \label{ss:sr}

\bib{\bibentry{TIS:Book2001}}
\note{\it
  The ``TIS'' book contains several variants of serializability:
  Final State Serializability, View Serializability, Conflict Serializability,
  Commit Serializability, Multiversion Serializability, Global Serializability,
  Quasi Serializability
}

\bib{\bibentry{ConsistencyAndLocking:CACM1976}}
\note{\it
  This paper defines the concepts of transaction, consistency and schedule
  and shows that consistency requires that a transaction
  cannot request new locks after releasing a lock.
  Then it is argued that a transaction needs to lock a logical rather than
  a physical subset of the database.
  These subsets may be specified by predicates.
  An implementation of predicate locks
  which satisfies the consistency condition is suggested.

  This is the first paper to formalize mathematically the concurrency control problem.
  It also defines ``conflict serializability'',
  which is termed DSR in~\cite{Papadimitriou:JACM1979}.
}

\bib{\bibentry{Papadimitriou:JACM1979}}
\note{\it
  It is shown that recognizing the transaction histories that are serializable
  is an NP-complete problem.
  Several efficiently recognizable subclasses are introduced.
}

\bib{\bibentry{DistributedLocking:PODS1982}}
\note{\it
  We examine the problem of determining whether a set of locked transactions,
  accessing a distributed database, is guaranteed to produce only serializable schedules.
  For a pair of transactions we prove that this concurrency control problem
  (which is polynomially solvable for centralized databases)
  is in general coNP-complete.
}

\bib{\bibentry{SerializabilityLocking:JACM1984}}
\note{\it
  It is shown that locking cannot achieve the full power of serializability.
  An exact characterization of the schedules that can be produced
  if locking is used to control concurrency is given for two versions of serializability:
  state serializability and view serializability.

  See also its STOC conference version~\cite{SerializabilityLocking:STOC1981}.
}

\bib{\bibentry{OneCopySR:TSE1984}}
\note{\it
  Introduce One Copy Serializability (1SR),
  as a distributed/replicated counterpart of Serializability in a single-server system.
}

\bib{\bibentry{DCC:SIAM1985}}
\note{\it
  We present a formal framework for distributed databases,
  and we study the complexity of the concurrency control problem in this framework.
  Our transactions are partially ordered sets of actions,
  as opposed to the straight-line programs of the centralized case.
  The concurrency control algorithm, or scheduler, is itself a distributed program.
}

\bib{\bibentry{MVSR:PODS1985}}
\note{\it
  In this paper we introduce a new notion of multiversion serializability (MVSR)
  based on conflicts (MVCSR), and discuss its relation with the well known
  single version conflict serializability (CSR).
}

\bib{\bibentry{SSI:VLDB2012}}
\note{\it
  This paper describes our experience implementing PostgreSQL’s
  new serializable isolation level.
  It is based on the recently-developed Serializable Snapshot Isolation (SSI) technique.
  This is the first implementation of SSI in a production database release
  as well as the first in a database that did not previously
  have a lock-based serializable isolation level.
}

\bib{\bibentry{CausalSR:Raynal1996}}
\note{\it
  It defines Causal Serializability.
}
%%%%%%%%%%%%%%%%%%%%
\subsection{Snapshot Isolation} \label{ss:si}

\bib{\bibentry{ROAnomalySI:SIGMODRecord2004}}
\note{\it
  It has been widely assumed that, under SI,
  read-only transactions always execute serializably
  provided the concurrent update transactions are serializable.
  The reason for this is that
  all SI reads return values from a single instant of time
  when all committed transactions have completed their writes
  and no writes of non-committed transactions are visible.
  This seems to imply that read-only transactions will not read anomalous results
  so long as the update transactions with which
  they execute do not write such results.
  In the current note, however, we exhibit an example
  contradicting these assumptions:
  it is possible for an SI history to be non-serializable
  while the sub-history containing all update transactions is serializable.
}

\bib{\bibentry{GSI:SRDS2005}}
\note{\it
  It defines GSI (Generalized Snapshot Isolation)
  and PSI (Perfix-Consistent Snapshot Isolation).

  While (conventional) snapshot isolation requires that transactions
  observe the ``latest'' snapshot of the database,
  generalized snapshot isolation allows the use of ``older'' snapshots,
  facilitating a replicated implementation.
}

\bib{\bibentry{CritiqueSI:EuroSys2012}}
\note{\it
  We introduce write-snapshot isolation, a novel isolation level
  that has a performance comparable with that of snapshot isolation,
  and yet provides serializability.
  The main insight in write-snapshot isolation is
  to prevent read-write conflicts in contrast to write-write conflicts
  that are prevented by snapshot isolation.
}

\bib{\bibentry{ClockSI:SRDS2013}}
\note{\it
  Clock-SI is a fully distributed protocol that implements snapshot isolation (SI)
  for partitioned data stores.
  It derives snapshot and commit timestamps from loosely synchronized clocks,
  rather than from a centralized timestamp authority as used in current systems.
}

\bib{\bibentry{DSI:VLDB2014}}
\note{\it
  This paper revisits the problem of implementing Snapshot Isolation
  in a distributed database system and makes three important contributions.
  First, a complete definition of Distributed Snapshot Isolation is given,
  thereby extending existing definitions from the literature.
  Based on this definition, a set of criteria is proposed to
  efficiently implement Snapshot Isolation in a distributed system.
  Second, the design space of alternative methods to
  implement Distributed Snapshot Isolation is presented based on this set of criteria.
  Third, a new approach to implement Distributed Snapshot Isolation is devised;
  we refer to this approach as Incremental.
}

\bib{\bibentry{AnalysingSI:JACM2018}}
\note{\it
  We give an alternative specification to SI
  that characterises it in terms of transactional dependency graphs of Adya et al.,
  generalising serialisation graphs.

  We then exploit our specification to obtain two kinds of static analyses.
  The first one checks when a set of transactions running under SI
  can be chopped into smaller pieces without introducing new behaviours,
  to improve performance.
  The other analysis checks whether a set of transactions running
  under a weakening of SI behaves the same as when running under SI.
}
%%%%%%%%%%%%%%%%%%%%
\subsection{Mixed Transactional Consistency Models} \label{ss:mixed}

\bib{\bibentry{AllocatingIsolation:PODS2015}}
\note{\it
  In this paper, we discuss the problem of taking a collection of transactions,
  and allocating each to run at an appropriate isolation level
  (and thus use a particular concurrency control mechanism),
  while still ensuring that every execution will be conflict serializable.
  When each transaction can use either S2PL, or snapshot isolation,
  we characterize exactly the acceptable allocations,
  and provide a simple graph-based algorithm
  which determines the weakest acceptable allocation.
}

\bib{\bibentry{MixT:PLDI2018}}
\note{\it
  To manipulate both weakly and strongly consistent data in a single transaction,
  we introduce a new abstraction: mixed-consistency transactions,
  embodied in a new embedded language, MixT.
  Programmers explicitly associate consistency models with remote storage sites;
  each atomic, isolated transaction can access a mixture of data
  with different consistency models.
}

\bib{\bibentry{fastPSI:MasterThesis2020}}
\note{\it
  This work shows an approach to bridge the gap between PSI, NMSI
  and strong consistency models like serialisability.
  It introduces and implements fastPSI,
  a consistency protocol that allows the user to selectively enforce serialisability
  for certain executions,
  while retaining the scalability properties of weaker consistency models like PSI and NMSI.
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Robustness (and Dependency Graphs)} \label{section:robustness}

\bib{\bibentry{Fekete:TODS2005}}
\note{\it
  This article develops a theory that characterizes
  when nonserializable executions of applications can occur under SI.
}

\bib{\bibentry{AutomatingSIAnomalies:VLDB2007}}
\note{\it
  In this paper we address several issues in
  extending the earlier theory to practical detection/correction of anomalies.
  We first show how to mechanically find a set of programs
  which is large enough so that
  we ensure that all executions will be free of SI anomalies,
  by modifying these programs appropriately.
  We then address the problem of false positives,
  i.e., transaction programs wrongly identified
  as possibly leading to anomalies,
  and present techniques that can significantly reduce such false positives.
  Unlike earlier work, our techniques are designed to be automated,
  rather than manually carried out.
  We describe a tool which we are developing to carry out this task.
}

\bib{\bibentry{SerialSI:TODS2009}}
\note{\it
  This article describes a modification to
  the concurrency control algorithm of a database management system that
  automatically detects and prevents snapshot isolation anomalies
  at runtime for arbitrary applications,
  thus providing serializable isolation.
}

\bib{\bibentry{ConsistencyAnamoalies:VLDB2014}}
\note{\it
  While it is fairly well known how to
  identify qualitatively the anomalies
  that are possible under a certain isolation level,
  it is much more difficult to detect and quantify such anomalies
  during run-time of a given application.
  In this paper, we present a new approach to
  detect and quantify consistency anomalies
  for arbitrary multi-tier application
  running under any isolation levels ensuring at least read committed.
}

\bib{\bibentry{RushMon:SIGMOD2018}}
\note{\it
  To resolve these problems, these systems need an indicator
  to report the number of bad events caused by out-of-order executions.
  In this paper, we tackle this problem.
  Based on transaction processing theory,
  we find the number of cycles in the dependency graph,
  and demonstrate it is a good indicator.
  With this observation,
  we propose the first real-time isolation anomalies monitor.
}

\bib{\bibentry{RobustTCC:CONCUR2019}}
\note{\it
  In this paper, we investigate application-specific relationships
  between several variations of causal consistency
  and we address the issue of verifying automatically
  if a given transactional program is robust against causal consistency,
  i.e., all its behaviors when executed over
  an arbitrary causally consistent database are serializable.
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Concurrency Control Protocols} \label{section:cc}

%%%%%%%%%%%%%%%%%%%%
\subsection{Theory} \label{ss:theory}

\bib{\bibentry{Optimality:SIGMOD1979}}
\note{\it
  There is a growing body of literature on various solutions
  to the concurrency control problem.
  This paper gives a uniform framework for evaluating these solutions,
  and, in many cases, for establishing their optimality.
  We point out a trade-off between the performance of a scheduler
  and the information that it uses.
  We show that most of the existing work on concurrency control
  is concerned with specific points of this fundamental trade-off.
  For example, our framework allows us to formally show that
  the popular approach of Serialization is the best one can hope for
  when only syntactic information is available.
}

\bib{\bibentry{Interval:VLDB1984}}
\note{\it
  Certification by Intervals of Timestamps in Distributed Database Systems.
}
%%%%%%%%%%%%%%%%%%%%
\subsection{Overview} \label{ss:cc-overview}

\bib{\bibentry{CC:CSUR1981}}
\note{\it
  In this paper we survey, consolidate, and present the state of the art
  in distributed database concurrency control.
  The heart of our analysts is a decomposition of the concurrency control problem
  into two major subproblems: read-write and write-write synchronization.
  We describe a series of synchronization techniques for solving each subproblem
  and show how to combine these techniques into algorithms
  for solving the entire concurrency control problem.
  Such algorithms are called ``concurrency control methods.''
  We describe 48 principal methods, including all practical algorithms
  that have appeared m the literature plus several new ones.
  We concentrate on the structure and correctness of concurrency control algorithms.
  Issues of performance are given only secondary treatment.
}
%%%%%%%%%%%%%%%%%%%%
\subsection{Locking} \label{ss:locking}

\bib{\bibentry{Locking:VLDB1975}}
\note{\it
  This paper proposes a locking protocol which associates locks with sets of resources.
  This protocol allows simultaneous locking at various granularities
  by different transactions.
  It is based on the introduction of additional lock modes
  besides the conventional share mode and exclusive mode.
  The protocol is generalized from simple hierarchies of locks
  to directed acyclic graphs of locks and to dynamic graphs of locks.
  The issues of scheduling and granting conflicting requests for the same resource
  are then discussed.
  Lastly, these ideas are compared with the lock mechanisms
  provided by existing data management systems.

  It introduces intention locks.
}

\bib{\bibentry{SafeLocking:JACM1982}}
\note{\it
  Necessary and sufficient conditions are found for a locking policy to be safe,
  but it is shown that in general it is NP-complete to test for these conditions.
  However, when the database has a given structure,
  a simple set of rules which is sufficient for safety and, moreover,
  necessary for a wide class of natural locking policies is developed.
}

\bib{\bibentry{Beyond2PL:JACM1985}}
\note{\it
  Graph protocols.
}

\bib{\bibentry{CCLocking:CSUR1998}}
\note{\it
  This tutorial reviews CC methods based on standard locking,
  restart-oriented locking methods, two-phase processing methods
  including optimistic CC, and hybrid methods
  (combining optimistic CC and locking) in centralized systems.
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Hybrid Concurrency Control}
\label{ss:hybrid-cc}

\bib{\bibentry{Salt:OSDI2014}}
\note{\it
  This paper presents Salt,
  a distributed database that allows developers
  to improve the performance and scalability of their ACID applications
  through the incremental adoption of the BASE approach.
  Salt’s motivation is rooted in the Pareto principle:
  for many applications, the transactions that
  actually test the performance limits of ACID are few.
  To leverage this insight, Salt introduces BASE transactions,
  a new abstraction that encapsulates the workflow of performance-critical transactions.
  BASE transactions retain desirable properties like atomicity and durability,
  but, through the new mechanism of Salt Isolation,
  control which granularity of isolation they offer to other transactions,
  depending on whether they are BASE or ACID.
}

\bib{\bibentry{MCC:SOSP2015}}
\note{\it
  This paper describes the design, implementation,
  and evaluation of Callas, a distributed database system that
  offers to unmodified, transactional ACID applications
  the opportunity to achieve a level of performance
  that can currently only be reached by
  rewriting all or part of the application in a BASE/NoSQL style.
  The key to combining performance and ease of programming
  is to decouple the ACID abstraction,
  which Callas offers identically for all transactions,
  from the mechanism used to support it.
  MCC, the new Modular approach to Concurrency Control
  at the core of Callas, makes it possible to partition transactions
  in groups with the guarantee that,
  as long as the concurrency control mechanism
  within each group upholds a given isolation property,
  that property will also hold among transactions in different groups.
}

\bib{\bibentry{ComposingCC:PLDI2015}}
\note{\it
  Concurrency control poses significant challenges
  when composing computations over multiple data-structures
  (objects) with different concurrency-control implementations.
  We formalize the usually desired requirements
  (serializability, abort-safety, deadlock-safety, and opacity)
  as well as stronger versions of these properties that enable composition.
  We show how to compose protocols satisfying these properties
  so that the resulting combined protocol also satisfies these properties.
  Our approach generalizes well-known protocols
  (such as two-phase-locking and two-phase-commit)
  and leads to new protocols.
  We apply this theory to show how we can safely
  compose optimistic and pessimistic concurrency control.
  For example, we show how we can execute a transaction
  that accesses two objects, one controlled by an STM and another by locking.
}

\bib{\bibentry{ModularCC:SIGMOD2017}}
\note{\it
  This paper presents Tebaldi, a distributed key-value store
  that explores new ways to harness the performance opportunity
  of combining different specialized concurrency control mechanisms (CCs)
  within the same database.
  Tebaldi partitions conflicts at a fine granularity
  and matches them to specialized CCs within a hierarchical framework
  that is modular, extensible, and able to
  support a wide variety of concurrency control techniques,
  from single-version to multiversion and from lock-based to timestamp-based.
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Atomic Commitment and Consensus} \label{ss:ac-consensus}

\bib{\bibentry{KnowledgeAC:PODS1987}}
\note{\it
  A knowledge-theoretic analysis of atomic commitment protocols.
}

\bib{\bibentry{PaxosCommit:TODS2006}}
\note{\it
  The Paxos Commit algorithm runs a Paxos consensus algorithm
  on the commit/abort decision of each participant
  to obtain a transaction commit protocol
  that uses 2F + 1 coordinators and makes progress
  if at least F + 1 of them are working properly.
}

\bib{\bibentry{UsingPaxos:VLDB2011}}
\note{\it
  Spinnaker is an experimental datastore
  that is designed to run on a large cluster of commodity servers
  in a single data center.
  It features key-based range partitioning, 3-way replication,
  and a transactional get-put API
  with the option to choose either strong or timeline consistency on reads.
  This paper describes Spinnaker’s Paxos-based replication protocol.
}

\bib{\bibentry{Megastore:CIDR2011}}
\note{\it
  Megastore is a storage system developed to
  meet the requirements of today’s interactive online services.
  Megastore blends the scalability of a NoSQL datastore
  with the convenience of a traditional RDBMS in a novel way,
  and provides both strong consistency guarantees and high availability.
  We provide fully serializable ACID semantics within
  fine-grained partitions of data.
  This partitioning allows us to synchronously
  replicate each write across a wide area network
  with reasonable latency and support seamless failover between datacenters.
  This paper describes Megastore’s semantics and replication algorithm.

  While many systems use Paxos solely for
  locking, master election, or replication of metadata and configurations,
  we believe that Megastore is the largest system deployed
  that uses Paxos to replicate primary user data across datacenters on every write.
}

\bib{\bibentry{NotSerial:VLDB2012}}
\note{\it
  We first develop and analyze a transaction management
  and replication protocol based on
  a straightforward implementation of the Paxos algorithm.
  Our investigation reveals that this protocol
  acts as a concurrency prevention mechanism
  rather than a concurrency control mechanism.
  We then propose an enhanced protocol
  called Paxos with Combination and Promotion (Paxos-CP)
  that provides true transaction concurrency
  while requiring the same per instance message complexity
  as the basic Paxos protocol.
  Finally, we compare the performance of Paxos and Paxos-CP
  in a multi-datacenter experimental study,
  and we demonstrate that Paxos-CP results in
  significantly fewer aborted transactions than basic Paxos.
}

\bib{\bibentry{ReplicatedCommit:VLDB2013}}
\note{\it
  Spanner uses Two-Phase Commit and Two-Phase Locking
  to provide atomicity and isolation for globally distributed data,
  running on top of Paxos to provide fault-tolerant log replication.
  We show in this paper that it is possible to
  provide the same ACID transactional guarantees
  for multi-datacenter databases
  with fewer cross-datacenter communication trips,
  compared to replicated logging.
  Instead of replicating the transactional log,
  we replicate the commit operation itself,
  by running Two-Phase Commit multiple times in different datacenters
  and using Paxos to reach consensus among datacenters
  as to whether the transaction should commit.
  Doing so not only replaces several inter-datacenter communication trips
  with intra-datacenter communication trips,
  but also allows us to integrate atomic commitment and isolation protocols
  with consistent replication protocols to
  further reduce the number of cross-datacenter communication trips
  needed for consistent replication;
  for example, by eliminating the need for an election phase in Paxos.
}

\bib{\bibentry{MDCC:EuroSys2013}}
\note{\it
  MDCC (Multi-Data Center Consistency)
  is an optimistic commit protocol for geo-replicated transactions,
  that does not require a master or static partitioning,
  and is strongly consistent at a cost similar to
  eventually consistent protocols.
  MDCC takes advantage of Generalized Paxos for transaction processing
  and exploits commutative updates with value constraints
  in a quorum-based system.
  Our experiments show that MDCC outperforms existing synchronous
  transactional replication protocols, such as Megastore,
  by requiring only a single message round-trip
  in the normal operational case independent of the master-location
  and by scaling linearly with the number of machines
  as long as transaction conflict rates permit.
}

\bib{\bibentry{CC+Consensus:OSDI2016}}
\note{\it
  In this paper, we make the key observation
  that the coordination required for concurrency control
  and consensus is highly similar.
  Specifically, each tries to ensure
  the serialization graph of transactions is acyclic.
  We exploit this insight in the design of Janus,
  a unified concurrency control and consensus protocol.

  Like MDCC and TAPIR,
  Janus can commit unconflicted transactions in this class
  in one round-trip.
  Unlike MDCC and TAPIR, Janus avoids aborts due to contention:
  it commits conflicted transactions in this class
  in at most two round-trips as long as
  the network is well behaved and a majority of each server replica is alive.
}

\bib{\bibentry{TCS:DISC2018}}
\note{\it
  We introduce Transaction Certification Service (TCS),
  a new formal problem that captures safety guarantees
  of multi-shot transaction commit protocols
  with integrated concurrency control.
  TCS is parameterized by a certification function
  that can be instantiated to support common isolation levels,
  such as serializability and snapshot isolation.
  We then derive a provably correct crash-resilient protocol
  for implementing TCS through successive refinement.
}

\bib{\bibentry{ReconfigurableTCS:PODC2019}}
\note{\it
  Existing TCS protocols require $2f+1$ crash-stop replicas
  per shard to tolerate $f$ failures.
  In this paper we present atomic commit protocols
  that require only $f+1$ replicas
  and reconfigure the system upon failures
  using an external reconfiguration service.
  We furthermore rigorously prove that these protocols
  correctly implement a recently proposed TCS specification.
  We present protocols in two different models---
  the standard asynchronous message-passing model
  and a model with Remote Direct Memory Access (RDMA),
  which allows a machine to access the memory of another machine
  over the network without involving the latter's CPU.
}

\bib{\bibentry{TAPIR:TOCS2018}}
\note{\it
  We present the Transactional Application Protocol
  for Inconsistent Replication (TAPIR),
  the first transaction protocol to use a novel replication protocol,
  called inconsistent replication,
  that provides fault tolerance without consistency.
  By enforcing strong consistency only in the transaction protocol,
  TAPIR can commit transactions in a single round-trip
  and order distributed transactions without centralized coordination.

  This article demonstrates that it is possible
  to build distributed transactions with better performance
  and strong consistency semantics
  by building on a replication protocol with no consistency.
  We present inconsistent replication,
  a new replication protocol that provides fault tolerance
  without consistency, and TAPIR,
  a new distributed transaction protocol
  that provides linearizable transactions using IR.

  (Fast path vs. Slow path)
}

\bib{\bibentry{VisibilityControl:VLDB2019}}
\note{\it
  We introduce Ocean Vista – a novel distributed protocol
  that guarantees strict serializability.
  We observe that concurrency control and replication
  address different aspects of resolving the visibility of transactions,
  and we address both concerns using a multi-version protocol
  that tracks visibility using version watermarks
  and arrives at correct visibility decisions using efficient gossip.
  Gossiping the watermarks enables asynchronous transaction processing
  and acknowledging transaction visibility in batches
  in the concurrency control and replication protocols,
  which improves efficiency under high cross-datacenter network delays.
}

\bib{\bibentry{Unifying:VLDB2019}}
\note{\it
  Data storage in the Cloud needs to be scalable and fault-tolerant.
  Atomic commitment protocols such as Two Phase Commit (2PC)
  provide ACID guarantees for transactional access
  to sharded data and help in achieving scalability.
  Whereas consensus protocols such as Paxos
  consistently replicate data across different servers
  and provide fault tolerance.
  Cloud based datacenters today typically
  treat the problems of scalability and fault-tolerance disjointedly.
  In this work, we propose a unification of these two different paradigms
  into one framework called Consensus and Commitment (C\&C) framework.
  The C\&C framework can model existing and well known data management protocols
  as well as propose new ones.
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Implementations} \label{ss:impl}

\bib{\bibentry{RSSI:VLDB2011}}
\note{\it
  We propose a new algorithm Replicated Serializable Snapshot Isolation (RSSI)
  that uses SI at each site, and combines this with a certification algorithm
  to guarantee 1-copy serializable global execution.
  Management of ww-conflicts is similar to what is done in 1-copy SI.
  But unlike previous designs for 1-copy serializable systems,
  we do not need to prevent all rw-conflicts among concurrent transactions.
  We formalize this in a theorem
  that shows that many rw-conflicts are indeed false positives
  that do not risk non-serializable behavior.
  Our proposed RSSI algorithm will only abort a transaction
  when it detects a well-defined pattern of two consecutive rw-edges
  in the serialization graph.
}

\bib{\bibentry{MaaT:VLDB2014}}
\note{\it
  Very little theoretical work has been done to
  entirely eliminate the need for locking in distributed transactions,
  including locks acquired during two-phase commit.
  In this paper, we re-design optimistic concurrency control to
  eliminate any need for locking even for atomic commitment,
  while handling the practical issues in earlier theoretical work
  related to this problem.
}

\bib{\bibentry{ConfluxDB:VLDB2014}}
\note{\it
  We propose a set of techniques that support update transaction execution
  over multiple partitioned sites,
  thereby allowing the master to scale.
  Our techniques determine a total SI order for update transactions
  over multiple master sites without requiring global coordination
  in the distributed system,
  and ensure that updates are installed in this order at all sites
  to provide consistent and scalable replication with SI.
}

\bib{\bibentry{CCOptimality:VLDB2018}}
\note{\it
  In this paper, we show that such ``latency-optimal'' ROTs
  induce an extra overhead on writes that is so high that
  it actually jeopardizes performance even in read-dominated workloads.
  We show this result from a practical as well as from a theoretical angle.
}

\bib{\bibentry{2PL+2PC:VLDB2019}}
\note{\it
  The goal of this paper is to establish a baseline for
  concurrency control mechanisms on thousands of cores
  connected through a low-latency network.
  We develop a distributed lock table
  supporting all the standard locking modes used in database engines.
  We focus on strong consistency in the form of strict serializability
  implemented through strict 2PL,
  but also explore read-committed and repeatable-read,
  two common isolation levels used in many systems.

  The surprising result is that, for TPC-C, 2PL and 2PC
  can be made to scale to thousands of cores and hundreds of machines,
  reaching a throughput of over 21 million transactions per second
  with 9.5 million New Order operations per second.

  To achieve these results, our implementation
  relies on Remote Direct Memory Access (RDMA).
}

\bib{\bibentry{SLOG:VLDB2019}}
\note{\it
  In this paper we discuss SLOG:
  a system that avoids this tradeoff for workloads
  which contain physical region locality in data access.
  SLOG achieves high-throughput, strictly serializable ACID transactions
  at geo-replicated distance
  and scale for all transactions submitted across the world,
  all the while achieving low latency for transactions
  that initiate from a location close to the home region for data they access.
}

\bib{\bibentry{Creek:arXiv2019}}
\note{\it
  In this paper we introduce Creek, a low-latency,
  eventually consistent replication scheme
  that also enables execution of strongly consistent operations
  (akin to ACID transactions).

  Creek totally-orders all operations,
  but does so using two different broadcast mechanisms:
  a timestamp-based one and our novel conditional atomic broadcast (CAB).
  The former is used to establish a tentative order
  of all operations for speculative execution,
  and it can tolerate network partitions.
  On the other hand, CAB is only used to ensure linearizable execution of
  the strongly consistent operations,
  whenever distributed consensus can be solved.
  The execution of strongly consistent operations
  also stabilizes the execution order of the causally
  related weakly consistent operations.
  Creek uses multiversion concurrency control
  to efficiently handle operations’ rollbacks
  and reexecutions resulting from the mismatch
  between the tentative and the final execution orders.
}

\bib{\bibentry{TCC:SIGMOD2020}}
\note{\it
  This raises the challenge of
  Multisite Transactional Causal Consistency (MTCC):
  the ability to provide causal consistency
  for all I/Os within a given transaction
  even if it runs across multiple physical sites.
  We present protocols for MTCC implemented in a system called HYDROCACHE.
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Theory: Complexity and Limitations}
\label{section:theory}

\bib{\bibentry{CannotFast:SPAA2019}}
\note{\it
  We prove that no fully transactional system
  can provide fast read transactions
  (including read-only ones that are considered
  the most frequent in practice).
  Specifically, to achieve fast read transactions,
  the system has to give up support of transactions
  that write more than one object.
  We prove this impossibility result for distributed storage systems
  that are causally consistent, i.e.,
  they do not require to ensure any strong form of consistency.
  Therefore, our result holds also for any system
  that ensures a consistency level stronger than causal consistency,
  e.g., strict serializability.
  The impossibility result holds even for systems
  that store only two objects
  (and support at least two servers and at least four clients).
  It also holds for systems that are partially replicated.
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Formal Methods} \label{section:formal-methods}

\bib{\bibentry{ProgramLogic2PL:TR2017}}
\note{\it
  We present a program logic for serializable transactions
  that are able to manipulate a shared storage.

  We show this by providing the first application of our logic
  in terms of the Two-phase locking (2pl) protocol which ensures serializability.
}

\bib{\bibentry{AloneTogether:POPL2018}}
\note{\it
  Unfortunately, the semantics of weak isolation is poorly understood,
  and usually explained only informally in terms of low-level implementation artifacts.
  Consequently, verifying high-level correctness properties in such environments
  remains a challenging problem.
  To address this issue, we present a novel program logic
  that enables compositional reasoning about the behavior of
  concurrently executing weakly-isolated transactions.
}

\bib{\bibentry{MixT:PLDI2018}}
\note{\it
  To manipulate both weakly and strongly consistent data
  in a single transaction, we introduce a new abstraction:
  mixed-consistency transactions,
  embodied in a new embedded language, MixT.
}

\bib{\bibentry{CoordAvoidance:VLDB2018}}
\note{\it
  In this paper, we establish conditions under which
  a commonly used sufficient condition for invariant confluence
  is both necessary and sufficient,
  and we use this condition to design
  (a) a general-purpose interactive invariant confluence decision procedure
  and (b) a novel sufficient condition that can be checked automatically.
  We then take a step beyond invariant confluence
  and introduce a generalization of invariant confluence,
  called segmented invariant confluence,
  that allows us to replicate non-invariant confluent objects
  with a small amount of coordination.
}

\bib{\bibentry{IPA:VLDB2018}}
\note{\it
  In this paper we propose a novel approach
  to preserve application invariants
  without coordinating the execution of operations.
  The approach consists of modifying operations
  in a way that application invariants are maintained
  in the presence of concurrent updates.
  When no conflicting updates occur,
  the modified operations present their original semantics.
  Otherwise, we use sensible and deterministic conflict resolution policies
  that preserve the invariants of the application.
  To implement this approach, we developed a static analysis, IPA,
  that identifies conflicting operations
  and proposes the necessary modifications to operations.
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Systems} \label{section:systems}

\bib{\bibentry{Percolator:OSDI2010}}
\note{\it
  We have built Percolator, a system for incrementally processing updates
  to a large data set, and deployed it to create the Google web search index.
}

\bib{\bibentry{Azure:SOSP2011}}
\note{\it
  Windows Azure Storage (WAS) is a cloud storage system that
  provides customers the ability to store
  seemingly limitless amounts of data for any duration of time.
  WAS customers have access to their data from anywhere at any time
  and only pay for what they use and store.
  In WAS, data is stored durably using both local and geographic replication
  to facilitate disaster recovery.
  Currently, WAS storage comes in the form of Blobs (files),
  Tables (structured storage), and Queues (message delivery).
  In this paper, we describe the WAS architecture, global namespace,
  and data model, as well as its resource provisioning,
  load balancing, and replication systems.
}

\bib{\bibentry{Spanner:TOCS2013}}
\note{\it
  Spanner is Google's scalable, multiversion,
  globally distributed, and synchronously replicated database.
  It is the first system to distribute data at global scale
  and support externally-consistent distributed transactions.
  This article describes how Spanner is structured, its feature set,
  the rationale underlying various design decisions,
  and a novel time API that exposes clock uncertainty.
  This API and its implementation are critical
  to supporting external consistency and a variety of powerful features:
  nonblocking reads in the past, lock-free snapshot transactions,
  and atomic schema changes, across all of Spanner.
}

\subsection{New Hardwares} \label{ss:hardware}

\bib{\bibentry{AvailabilityRDMA:VLDB2019}}
\note{\it
  In this paper, we first make the case that
  in modern RDMA-enabled networks,
  the bottleneck has shifted to CPUs,
  and therefore the existing network-optimized replication techniques
  are no longer optimal.

  We present Active-Memory Replication,
  a new high availability scheme that efficiently leverages RDMA
  to completely eliminate the processing redundancy in replication.
  Using Active-Memory, all replicas dedicate their processing power
  to executing new transactions,
  as opposed to performing redundant computation.
  Active-Memory maintains high availability and correctness
  in the presence of failures through an efficient RDMA-based undo-logging scheme.
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Testing} \label{section:testing}

\bib{\bibentry{RushMon:SIGMOD2018}}
\note{\it
  $\dots$ these systems need an indicator to report
  the number of ``bad event'' caused by ``out-of-order'' executions.
  In this paper, we tackle this problem.
  Based on transaction processing theory,
  we find the number of cycles in the dependency graph,
  and demonstrate it is a good indicator.
  With this observation,
  we propose the first real-time isolation anomalies monitor.
}

\bib{\bibentry{CLOTHO:OOPSLA2019}}
\note{\it
  This paper presents a novel testing framework for detecting serializability violations
  in (SQL) database-backed Java applications executing on weakly-consistent storage systems.
  We manifest our approach in a tool, CLOTHO, that combines a static analyzer
  and model checker to generate abstract executions,
  discover serializability violations in these executions,
  and translate them back into concrete test inputs suitable for deployment
  in a test environment.
}

\bib{\bibentry{ComplexityTx:OOPSLA2019}}
\note{\it
  In this work, we investigate the problem of checking
  whether a given execution of a transactional database adheres to some consistency model.
  We show that consistency models like read committed, read atomic, and causal consistency
  are polynomial-time checkable while prefix consistency and snapshot isolation are NP-complete in general.
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{../acm-sigchi}
\nobibliography{ab-tx}

\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%